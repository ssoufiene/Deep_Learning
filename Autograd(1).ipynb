{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        " # Sigmoid implementation"
      ],
      "metadata": {
        "id": "HP8SGeFAnpEX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.autograd import Function\n",
        "from torch.autograd import Variable"
      ],
      "metadata": {
        "id": "j6Sxc3A-D4-D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#pure python\n",
        "\n",
        "import numpy as np\n",
        "class Sigmoid :\n",
        "  def forward(self,x):\n",
        "    self.x=x\n",
        "    return(1/1+np.exp(-x))\n",
        "  def backward(self,grad):\n",
        "    grad_input=self.x * (1+self.x) * grad\n",
        "    return grad_input\n"
      ],
      "metadata": {
        "id": "YEjS0PkInzg_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#using autograd\n",
        "\n",
        "\n",
        "class Sigmoid:\n",
        "  @staticmethod\n",
        "  def forward(ctx,x):\n",
        "    output=1/(1+torch.exp(-x))\n",
        "    ctx.save_for_backward(output)\n",
        "    return output\n",
        "  @staticmethod\n",
        "  def backward(ctx,grad_output):\n",
        "    output, = ctx.saved_tensors\n",
        "    grad_x= output*(1-output) * grad_output\n",
        "    return grad_x\n",
        "\n"
      ],
      "metadata": {
        "id": "7q4mkYQPp-nq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Relu implementation"
      ],
      "metadata": {
        "id": "OxtfNj-R8BTR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ReLU(torch.autograd.Function):\n",
        "  @staticmethod\n",
        "  def forward(ctx,x):\n",
        "    ctx.save_for_backward(x)\n",
        "    return x.clamp(min=0)\n",
        "  def backward(ctx,grad_output):\n",
        "    x,=ctx.saved_tensors\n",
        "    grad_x=grad_output.clone()\n",
        "    grad_x[x<0]=0\n",
        "    return grad_x\n"
      ],
      "metadata": {
        "id": "kC-Ii44TzcfT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Example for relu\n",
        "x = Variable(torch.randn(3, 3), requires_grad=True)\n",
        "relu=ReLU.apply\n",
        "print(x)\n",
        "# Initialize the custom ReLU function\n",
        "relu = ReLU.apply\n",
        "# Forward pass\n",
        "output = relu(x)\n",
        "print(\"Output after forward pass:\")\n",
        "print(output)\n",
        "grad_output = torch.randn(3, 3)\n",
        "print(grad_output)\n",
        "output.backward(grad_output)\n",
        "grad_x = x.grad\n",
        "print(\"\\nGradient with respect to x:\")\n",
        "print(grad_x)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VZlpvL4K41a1",
        "outputId": "81b516a2-574e-46dd-ec84-12e333fb620c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[-1.3286, -0.3257,  0.6541],\n",
            "        [ 0.4730,  0.1508,  1.6782],\n",
            "        [ 0.4746,  0.9059,  0.0912]], requires_grad=True)\n",
            "Output after forward pass:\n",
            "tensor([[0.0000, 0.0000, 0.6541],\n",
            "        [0.4730, 0.1508, 1.6782],\n",
            "        [0.4746, 0.9059, 0.0912]], grad_fn=<ReLUBackward>)\n",
            "tensor([[-0.0046, -0.0621,  0.0312],\n",
            "        [-1.6013,  0.5070, -0.0314],\n",
            "        [ 0.2353,  1.3479,  0.7803]])\n",
            "\n",
            "Gradient with respect to x:\n",
            "tensor([[ 0.0000,  0.0000,  0.0312],\n",
            "        [-1.6013,  0.5070, -0.0314],\n",
            "        [ 0.2353,  1.3479,  0.7803]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Arctanh implementation"
      ],
      "metadata": {
        "id": "ItNupPj_-Yx9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Arctanh(Function):\n",
        "    @staticmethod\n",
        "    def forward(ctx, x):\n",
        "        ctx.save_for_backward(x)\n",
        "        r = (torch.log(1 + x) - torch.log(1 - x)) * 0.5\n",
        "        return r  # Return the computed result\n",
        "\n",
        "    @staticmethod\n",
        "    def backward(ctx, grad_output):\n",
        "        x, = ctx.saved_tensors\n",
        "        x_grad = grad_output / (1 - x**2)\n",
        "        return x_grad"
      ],
      "metadata": {
        "id": "pLdhbTnN-VYT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#testing code with gradcheck\n",
        "xT = torch.abs(torch.tensor([[0.11, 0.19, 0.57]], requires_grad=True)).type(torch.DoubleTensor)\n",
        "f=Arctanh.apply\n",
        "test=torch.autograd.gradcheck(lambda t:f(t),xT)\n",
        "print(test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PlS5L2aUBk-t",
        "outputId": "afcbe252-519e-40e0-adcb-2fa1f6ef7eda"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "True\n"
          ]
        }
      ]
    }
  ]
}